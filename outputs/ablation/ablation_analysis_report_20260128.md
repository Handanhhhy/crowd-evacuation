# 消融实验完整分析报告

**生成时间**: 2026-01-28
**实验批次**: 4次运行 (20260128_123245, 20260128_145151, 20260128_152739, 20260128_162201)

---

## 1. 实验概述

### 1.1 实验完成度

| 批次 | 时间戳 | 内容 | 状态 |
|------|--------|------|------|
| 1 | 20260128_123245 | A1, B1-B2, C1, D1-D2, E1-E2 | 部分完成 |
| 2 | 20260128_145151 | A组补跑: A1-A3 | ✅ 完成 |
| 3 | 20260128_152739 | B组补跑: B3-B6 | ✅ 完成 |
| 4 | 20260128_162201 | C2, D3-D4 | ✅ 完成 |

### 1.2 实验配置

```yaml
训练配置:
  total_timesteps: 30000
  n_envs: 4
  learning_rate: 0.0003

评估配置:
  n_eval_episodes: 5
  random_seeds: [42, 123]

环境配置:
  num_pedestrians: 80
  max_steps: 600
```

---

## 2. 分组实验结果

### 2.1 A组：观测空间消融（PPO Observation Space Ablation）

**目的**: 测试不同观测维度对PPO策略性能的影响

| 实验ID | 观测维度 | 疏散率 | 疏散时间(步) | 最大拥堵 | 累计奖励 |
|--------|----------|--------|--------------|----------|----------|
| A1_16D | 16D (全部) | 98.75-99% | 516-600 | 0.96-1.02 | 1138-1157 |
| **A2_8D** | **8D (精简)** | **99-99.5%** | **361-517** | 1.03-1.22 | **1205-1263** |
| A3_6D | 6D (最小) | 98.5-99% | 514-600 | 0.92-1.04 | 1110-1195 |

**8D观测空间包含的特征**:
- exit_density (3维)
- exit_congestion (3维)
- remaining_ratio (1维)
- time_ratio (1维)

**被移除的特征** (噪声):
- flow_direction
- evacuation_rate
- bottleneck_density

#### 关键发现

> 🎯 **8D观测空间表现最佳，超过16D完整观测！**
> - 疏散时间缩短 14-40%
> - 累计奖励提升 4-9%
> - 说明部分观测特征是噪声，干扰了策略学习

---

### 2.2 B组：奖励函数消融（PPO Reward Function Ablation）

**目的**: 测试各奖励组件对训练效果的贡献

| 实验ID | 移除的奖励组件 | 疏散率 | 疏散时间(步) | 累计奖励 | 分析 |
|--------|----------------|--------|--------------|----------|------|
| B1_full | 无 (基线) | 98.75-99% | 516-600 | 1138-1157 | 基线 |
| B2_no_evac | evac_per_person | 99-99.5% | 361-517 | 17-69 | 奖励低但疏散正常 |
| B3_no_congestion | congestion_penalty | 98.75-99% | 516-600 | 1287-1316 | 奖励更高 |
| **B4_no_balance** | **balance_penalty** | **99-99.5%** | **361-517** | **1355-1379** | **最佳** |
| B5_no_flow | flow_efficiency_bonus | 98.5-99% | 514-600 | 1110-1195 | 略降 |
| B6_no_safety | safety_distance_bonus | 99-99.25% | 457-532 | 931-992 | 贡献大 |

#### 关键发现

> 🎯 **balance_penalty 过度惩罚，移除后性能最优**
> - B4_no_balance 累计奖励最高 (1355-1379)
> - 疏散时间最短 (361-517步)

> ⚠️ **evac_per_person 对疏散率影响不大**
> - B2移除后疏散率反而略高
> - 说明SFM本身驱动疏散，不需要额外奖励

> ✅ **safety_distance_bonus 贡献显著**
> - B6移除后累计奖励大幅下降
> - 应保留此奖励组件

---

### 2.3 C组：轨迹预测消融（Trajectory Prediction Ablation）

**目的**: 比较神经网络与线性外推的轨迹预测效果

| 实验ID | 预测方法 | 疏散率 | 疏散时间(步) | 最大拥堵 | 累计奖励 |
|--------|----------|--------|--------------|----------|----------|
| C1_neural | Social-LSTM | 98.75-99% | 516-600 | 0.96-1.02 | 1138-1157 |
| C2_linear | 线性外推 | 98.75-99% | 516-600 | 0.96-1.02 | 1138-1157 |

#### 关键发现

> ⚠️ **神经网络轨迹预测无明显优势**
> - 两种方法性能几乎完全相同
> - 线性外推计算成本更低
> - 建议：使用线性外推替代神经网络

---

### 2.4 D组：行人仿真消融（Pedestrian Simulation Ablation）

**目的**: 测试SFM参数和行人类型对仿真效果的影响

| 实验ID | 配置 | 疏散率 | 疏散时间(步) | 最大拥堵 | 累计奖励 |
|--------|------|--------|--------------|----------|----------|
| **D1_full** | 完整SFM+多类型 | **99-99.25%** | 435-467 | 0.90-1.05 | 1197-1228 |
| D2_single | 单一类型行人 | 98.5-98.75% | 600 | **0.39-0.45** | 1243-1270 |
| D3_weak | 弱社会力(A=1000) | 99% | 509-530 | 0.90-1.00 | 1180-1197 |
| D4_no_gbm | 无GBM修正 | 98.75-99.25% | 466-600 | 0.82-0.90 | 1127-1229 |

#### 关键发现

> ✅ **D1_full (完整配置) 表现最均衡**
> - 疏散率最高
> - 疏散时间最短

> 📊 **多类型行人增加拥堵但更真实**
> - D2单一类型拥堵度最低(0.39-0.45)
> - 但疏散率略低，时间更长

> ⚠️ **GBM修正对性能影响不大**
> - D4_no_gbm与D1差异不显著
> - 可考虑简化

---

### 2.5 E组：引导策略消融（Guidance Strategy Ablation）

**目的**: 验证PPO引导策略的有效性

| 实验ID | 策略 | 疏散率 | 疏散时间(步) | 最大拥堵 | 累计奖励 |
|--------|------|--------|--------------|----------|----------|
| E1_ppo_guidance | PPO引导 | 98.75-99% | 516-600 | 0.96-1.02 | 1138-1157 |
| **E2_no_guidance** | **无引导(自由选择)** | **99.25%** | **428-463** | 0.94-1.11 | **1154-1252** |

#### 关键发现

> 🔴 **重要：PPO引导策略反而降低性能！**
> - E2无引导的疏散率更高 (99.25% vs 98.75-99%)
> - E2疏散时间更短 (428-463 vs 516-600步)
> - E2累计奖励更高

**可能原因**:
1. 训练步数不足 (30k步可能未收敛)
2. 观测空间包含噪声特征
3. 奖励函数设计不合理 (balance_penalty过度惩罚)

---

## 3. 综合分析

### 3.1 各组件重要性排序

| 排名 | 组件 | 影响程度 | 建议 |
|------|------|----------|------|
| 1 | 观测空间维度 | ⭐⭐⭐⭐⭐ | 使用8D替代16D |
| 2 | balance_penalty | ⭐⭐⭐⭐ | 移除或大幅降低 |
| 3 | PPO引导策略 | ⭐⭐⭐⭐ | 需重新训练优化 |
| 4 | 轨迹预测方法 | ⭐⭐ | 可用线性外推 |
| 5 | GBM修正 | ⭐ | 影响不大 |

### 3.2 最优配置推荐

```yaml
# 基于消融实验的最优配置

observation:
  # 8D观测空间 (A2_8D结论)
  exit_density: true
  exit_congestion: true
  flow_direction: false      # 移除
  evacuation_rate: false     # 移除
  bottleneck_density: false  # 移除
  remaining_ratio: true
  time_ratio: true

reward_weights:
  # B组结论
  evac_per_person: 12.0
  congestion_penalty: 3.0
  time_penalty: 0.2
  completion_bonus: 200.0
  balance_penalty: 0.0       # 移除 (B4结论)
  flow_efficiency_bonus: 1.5
  safety_distance_bonus: 0.5 # 保留 (B6结论)
  guidance_penalty: 0.3
  evacuation_rate_bonus: 2.0

trajectory_prediction:
  # C组结论
  method: "linear"           # 替代neural

pedestrian_simulation:
  # D组结论
  use_multi_type: true       # 保持多类型
  sfm_A: 2000.0             # 标准社会力
  gbm_enabled: true          # 可选
```

---

## 4. 优化建议

### 4.1 立即可做 (高优先级)

1. **采用8D观测空间**
   - 修改默认配置，移除噪声特征
   - 预期提升：疏散时间缩短14-40%

2. **移除balance_penalty**
   - 设置 `balance_penalty: 0.0`
   - 预期提升：累计奖励提升15-20%

3. **使用线性外推轨迹预测**
   - 替代Social-LSTM
   - 预期：相同性能，计算更快

### 4.2 中期优化 (需要重新训练)

1. **重新训练PPO策略**
   - 使用优化后的8D观测空间
   - 使用优化后的奖励函数
   - 增加训练步数到50k-100k

2. **验证优化效果**
   - 运行E组对比实验
   - 期望PPO引导优于无引导

### 4.3 长期研究方向

1. **探索其他RL算法**
   - TD3、SAC、MAPPO
   - 可能比PPO更适合此任务

2. **多出口协调优化**
   - 当前balance_penalty无效
   - 需要更好的出口均衡机制

---

## 5. 附录

### 5.1 实验数据文件位置

```
outputs/ablation/
├── 20260128_123245/    # 第一批
├── 20260128_145151/    # A组补跑
├── 20260128_152739/    # B组补跑
├── 20260128_162201/    # C/D组补跑
└── ablation_analysis_report.md  # 本报告
```

### 5.2 关键指标说明

| 指标 | 说明 | 理想值 |
|------|------|--------|
| evacuation_rate | 疏散完成率 | 越高越好 (>98%) |
| evacuation_time | 平均疏散时间 | 越低越好 (<500步) |
| max_congestion | 最大拥堵度 | 越低越好 (<1.0) |
| cumulative_reward | 累计奖励 | 越高越好 |
| exit_balance | 出口均衡度(std) | 越低越好 |

### 5.3 统计注意事项

- 每个实验运行2个随机种子 (42, 123)
- 每个种子评估5个episodes
- 部分实验疏散时间=600步表示达到max_steps超时
- final_mean_reward记录存在bug，均为0.00

---

*报告生成工具: Claude Code*
*项目: crowd-evacuation*
