# 快速验证结果报告

**时间**: 2026-01-22
**配置**: 30人, 20k步训练

## 结果数据

| 指标 | PPO引导 | 无引导(随机) | 差异 |
|------|---------|-------------|------|
| 疏散率 | 98.9% | 100.0% | -1.1% |
| 平均步数 | 334 | 206 | +128 |
| 平均奖励 | 476.6 | 605.2 | -128.6 |

## 问题分析

### 核心发现
**随机选择达到100%疏散率** = 当前场景太简单，不需要智能引导

### 原因分析
1. **行人数太少(30人)** - 不会形成拥堵，任意出口都能顺利疏散
2. **时间限制太宽松(400步)** - 足够让所有人自然疏散
3. **PPO反而有害** - 引导动作可能干扰了行人的自然选择

### PPO步数多的原因
PPO可能在学习一个"过度干预"的策略，频繁改变行人目标，导致：
- 行人来回切换目标出口
- 浪费时间在路径调整上
- 反而比"自由选择"更慢

## 结论

**当前场景不适合验证PPO价值** - 需要增加难度

## 下一步建议

增加场景难度，让随机选择**无法**达到100%疏散：

| 参数 | 当前值 | 建议值 | 目的 |
|------|--------|--------|------|
| 行人数 | 30 | **60-80** | 制造拥堵 |
| 最大步数 | 400 | **300** | 增加时间压力 |
| 训练步数 | 20k | 30k | 适应更难场景 |

预期效果：随机选择疏散率降到70-85%，PPO才有优化空间
