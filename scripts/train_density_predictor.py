#!/usr/bin/env python
"""
å¯†åº¦åœºé¢„æµ‹æ¨¡å‹è®­ç»ƒè„šæœ¬

æ­¥éª¤ï¼š
1. è¿è¡ŒSFMä»¿çœŸæ”¶é›†æ•°æ®ï¼ˆå¤šä¸ªepisodeï¼‰
2. æ„å»ºåºåˆ—æ•°æ®é›†
3. è®­ç»ƒConvLSTM
4. ä¿å­˜æ¨¡å‹åˆ° outputs/models/density_predictor.pt

ä½¿ç”¨æ–¹æ³•:
    # æ”¶é›†æ•°æ®å¹¶è®­ç»ƒ
    python scripts/train_density_predictor.py --collect-data --train
    
    # ä»…æ”¶é›†æ•°æ®
    python scripts/train_density_predictor.py --collect-data --n-episodes 20
    
    # ä½¿ç”¨å·²æœ‰æ•°æ®è®­ç»ƒ
    python scripts/train_density_predictor.py --train --epochs 100
    
    # è¯„ä¼°æ¨¡å‹
    python scripts/train_density_predictor.py --evaluate --model-path outputs/models/density_predictor.pt

å‚è€ƒæ–‡æ¡£: docs/new_station_plan.md å¯†åº¦åœºé¢„æµ‹æ¨¡å— TODO
"""

import argparse
import sys
from pathlib import Path
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from tqdm import tqdm
import json
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / "src"))

from prediction import (
    DensityFieldPredictor,
    DensityDataCollector,
    DensityPredictorNet,
    DensityPredictorLite,
    GRID_SIZE,
    CELL_SIZE,
    MAX_SAFE_DENSITY,
)
from prediction.data_collector import create_dataloader
from simulation.large_station_env import LargeStationEnv


def collect_training_data(
    n_episodes: int = 10,
    flow_level: str = "small",
    max_steps: int = 3000,
    save_dir: str = "outputs/training_data",
    dt: float = 0.1,
    collect_interval: int = 5,
    use_gpu_sfm: bool = True,
    resume: bool = True,
) -> DensityDataCollector:
    """æ”¶é›†è®­ç»ƒæ•°æ®
    
    Args:
        n_episodes: episodeæ•°é‡
        flow_level: äººæµé‡ç­‰çº§ (small/medium/large)
        max_steps: æ¯ä¸ªepisodeæœ€å¤§æ­¥æ•°
        save_dir: æ•°æ®ä¿å­˜ç›®å½•
        dt: ä»¿çœŸæ—¶é—´æ­¥é•¿
        collect_interval: æ•°æ®æ”¶é›†é—´éš”ï¼ˆæ¯Næ­¥æ”¶é›†ä¸€æ¬¡ï¼Œé»˜è®¤5æ­¥=0.5ç§’ï¼‰
        use_gpu_sfm: æ˜¯å¦ä½¿ç”¨GPUåŠ é€ŸSFMï¼ˆå¤§å¹…æå‡é€Ÿåº¦ï¼‰
        resume: æ˜¯å¦ä»æ–­ç‚¹ç»­è®­ï¼ˆè‡ªåŠ¨è·³è¿‡å·²å®Œæˆçš„episodeï¼‰
        
    Returns:
        DensityDataCollector: æ•°æ®æ”¶é›†å™¨
    """
    print("=" * 60)
    print("æ”¶é›†è®­ç»ƒæ•°æ®")
    print("=" * 60)
    
    # æ£€æŸ¥å·²å®Œæˆçš„episodesï¼ˆæ–­ç‚¹ç»­è®­ï¼‰
    save_path = Path(save_dir)
    save_path.mkdir(parents=True, exist_ok=True)
    
    existing_episodes = []
    if resume:
        for ep_dir in sorted(save_path.iterdir()):
            if ep_dir.is_dir() and (ep_dir / "frames.pkl").exists():
                # è§£æepisodeåç§°
                name = ep_dir.name
                if name.startswith(f"episode_") and name.endswith(f"_{flow_level}"):
                    try:
                        ep_num = int(name.split("_")[1])
                        existing_episodes.append(ep_num)
                    except:
                        pass
    
    existing_episodes = sorted(existing_episodes)
    start_episode = 0
    
    if existing_episodes:
        # æ‰¾åˆ°æœ€å¤§çš„è¿ç»­episode
        for i in range(len(existing_episodes)):
            if existing_episodes[i] == i:
                start_episode = i + 1
            else:
                break
        
        if start_episode > 0:
            print(f"\nğŸ”„ æ–­ç‚¹ç»­è®­æ¨¡å¼")
            print(f"  - å·²å®Œæˆ: {start_episode} ä¸ªepisode")
            print(f"  - å‰©ä½™: {n_episodes - start_episode} ä¸ªepisode")
            
            if start_episode >= n_episodes:
                print(f"\nâœ… æ‰€æœ‰ {n_episodes} ä¸ªepisodeå·²å®Œæˆï¼Œæ— éœ€ç»§ç»­æ”¶é›†")
                # åˆ›å»ºcollectorå¹¶åŠ è½½æ•°æ®
                exits = [{'id': f'exit_{i}', 'position': np.array([0, 0])} for i in range(8)]
                collector = DensityDataCollector(exits=exits, save_dir=save_dir)
                collector.load_all_episodes()
                return collector
    
    print(f"\né…ç½®:")
    print(f"  - Episodes: {n_episodes} (ä»ç¬¬{start_episode}ä¸ªå¼€å§‹)")
    print(f"  - äººæµç­‰çº§: {flow_level}")
    print(f"  - æœ€å¤§æ­¥æ•°: {max_steps}")
    print(f"  - æ”¶é›†é—´éš”: æ¯{collect_interval}æ­¥ ({collect_interval * dt:.1f}ç§’)")
    print(f"  - GPUåŠ é€ŸSFM: {use_gpu_sfm}")
    print(f"  - æ•°æ®ä¿å­˜: {save_dir}")
    
    # æ€§èƒ½ä¼°ç®—
    flow_config = {
        "small": 1000,
        "medium": 2000,
        "large": 3000,
    }
    n_peds = flow_config.get(flow_level, 1000)
    
    # ä¼°ç®—æ¯æ­¥æ—¶é—´ï¼ˆCPUçº¦2-3ç§’/æ­¥ï¼ŒGPUçº¦0.1-0.3ç§’/æ­¥ï¼‰
    if use_gpu_sfm:
        est_time_per_step = 0.2  # GPUåŠ é€Ÿ
        speed_note = "GPUåŠ é€Ÿ"
    else:
        est_time_per_step = 2.5  # CPU
        speed_note = "CPUï¼ˆå»ºè®®ä½¿ç”¨--use-gpu-sfmåŠ é€Ÿï¼‰"
    
    remaining_episodes = n_episodes - start_episode
    total_steps = remaining_episodes * max_steps
    est_total_time = total_steps * est_time_per_step / 3600  # å°æ—¶
    
    print(f"\næ€§èƒ½ä¼°ç®— ({speed_note}):")
    print(f"  - é¢„è®¡æ¯æ­¥æ—¶é—´: ~{est_time_per_step:.1f}ç§’")
    print(f"  - å‰©ä½™æ€»æ—¶é—´: ~{est_total_time:.1f}å°æ—¶")
    if not use_gpu_sfm:
        print(f"  - ğŸ’¡ æç¤º: ä½¿ç”¨ --use-gpu-sfm å¯æé€Ÿ10-20å€ï¼")
    print()
    
    # åˆ›å»ºç¯å¢ƒ
    env = LargeStationEnv(
        flow_level=flow_level,
        max_steps=max_steps,
        dt=dt,
        emergency_mode=True,
        use_gpu_sfm=use_gpu_sfm,
    )
    
    # æå–å‡ºå£ä¿¡æ¯
    exits = [{'id': e.id, 'position': e.position.copy()} for e in env.exits]
    
    # åˆ›å»ºæ•°æ®æ”¶é›†å™¨
    collector = DensityDataCollector(
        exits=exits,
        save_dir=save_dir,
    )
    
    for episode_idx in range(start_episode, n_episodes):
        print(f"\n[Episode {episode_idx + 1}/{n_episodes}]")
        
        collector.start_episode({
            'episode': episode_idx,
            'flow_level': flow_level,
            'max_steps': max_steps,
            'dt': dt,
        })
        
        obs, _ = env.reset()
        step = 0
        
        pbar = tqdm(total=max_steps, desc=f"Episode {episode_idx + 1}")
        
        while step < max_steps:
            # éšæœºåŠ¨ä½œï¼ˆç”¨äºæ•°æ®æ”¶é›†ï¼Œä¸éœ€è¦ç­–ç•¥ï¼‰
            action = env.action_space.sample()
            obs, reward, terminated, truncated, info = env.step(action)
            
            # æ”¶é›†æ•°æ®
            if step % collect_interval == 0:
                # è·å–è¡Œäººæ•°æ®
                pedestrians = []
                for ped in env.sfm.pedestrians:
                    pedestrians.append({
                        'position': ped.position.copy(),
                        'velocity': ped.velocity.copy(),
                    })
                
                timestamp = step * dt
                collector.collect_frame(pedestrians, timestamp)
            
            step += 1
            pbar.update(1)
            
            if terminated:
                break
        
        pbar.close()
        
        # ä¿å­˜episode
        episode_name = f"episode_{episode_idx:04d}_{flow_level}"
        collector.save_episode(episode_name)
        collector.end_episode()
        
        print(f"  ç–æ•£: {info.get('evacuated', 0)}äºº")
        print(f"  å‰©ä½™: {info.get('remaining', 0)}äºº")
        print(f"  æ”¶é›†å¸§æ•°: {len(collector.episodes[-1]) if collector.episodes else 0}")
    
    env.close()
    
    print("\n" + "=" * 60)
    print("æ•°æ®æ”¶é›†å®Œæˆ")
    stats = collector.get_statistics()
    print(f"  - Episodes: {stats['n_episodes']}")
    print(f"  - æ€»å¸§æ•°: {stats['total_frames']}")
    print("=" * 60)
    
    return collector


def train_model(
    data_dir: str = "outputs/training_data",
    model_save_path: str = "outputs/models/density_predictor.pt",
    epochs: int = 50,
    batch_size: int = 32,
    learning_rate: float = 1e-3,
    seq_length: int = 10,
    pred_horizon: int = 50,
    use_lite_model: bool = False,
    device: str = "auto",
) -> nn.Module:
    """è®­ç»ƒå¯†åº¦é¢„æµ‹æ¨¡å‹
    
    Args:
        data_dir: è®­ç»ƒæ•°æ®ç›®å½•
        model_save_path: æ¨¡å‹ä¿å­˜è·¯å¾„
        epochs: è®­ç»ƒè½®æ•°
        batch_size: æ‰¹æ¬¡å¤§å°
        learning_rate: å­¦ä¹ ç‡
        seq_length: è¾“å…¥åºåˆ—é•¿åº¦ï¼ˆ10å¸§ = 1ç§’ï¼‰
        pred_horizon: é¢„æµ‹æ­¥é•¿ï¼ˆ50å¸§ = 5ç§’ï¼‰
        use_lite_model: æ˜¯å¦ä½¿ç”¨è½»é‡çº§æ¨¡å‹
        device: è®­ç»ƒè®¾å¤‡
        
    Returns:
        è®­ç»ƒå¥½çš„æ¨¡å‹
    """
    print("=" * 60)
    print("è®­ç»ƒå¯†åº¦é¢„æµ‹æ¨¡å‹")
    print("=" * 60)
    
    # è®¾å¤‡æ£€æµ‹å’Œæ‰“å°
    print("\nè®¾å¤‡ä¿¡æ¯:")
    print(f"  - PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"  - CUDAå¯ç”¨: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"  - CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print(f"  - GPUæ•°é‡: {torch.cuda.device_count()}")
        for i in range(torch.cuda.device_count()):
            print(f"  - GPU {i}: {torch.cuda.get_device_name(i)}")
            print(f"    - æ˜¾å­˜: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.2f} GB")
    
    if device == "auto":
        device = "cuda" if torch.cuda.is_available() else "cpu"
    
    print(f"\n  - ä½¿ç”¨è®¾å¤‡: {device}")
    if device == "cuda":
        print(f"  - å½“å‰GPU: {torch.cuda.get_device_name(0)}")
    print()
    
    # åŠ è½½æ•°æ®
    # é¦–å…ˆåˆ›å»ºä¸€ä¸ªä¸´æ—¶çš„collectoræ¥è·å–exitsä¿¡æ¯
    data_path = Path(data_dir)
    if not data_path.exists():
        raise FileNotFoundError(f"è®­ç»ƒæ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
    
    # ä»ç¬¬ä¸€ä¸ªepisodeè·å–å‡ºå£ä¿¡æ¯
    first_episode = None
    for ep_dir in sorted(data_path.iterdir()):
        if ep_dir.is_dir() and (ep_dir / "frames.pkl").exists():
            first_episode = ep_dir
            break
    
    if first_episode is None:
        raise FileNotFoundError(f"æœªæ‰¾åˆ°æœ‰æ•ˆçš„è®­ç»ƒæ•°æ®")
    
    # è¯»å–ç¬¬ä¸€å¸§è·å–å‡ºå£è·ç¦»åœº
    import pickle
    with open(first_episode / "frames.pkl", 'rb') as f:
        frames_data = pickle.load(f)
    
    # åˆ›å»ºè™šæ‹Ÿexitsåˆ—è¡¨ï¼ˆä»…ç”¨äºåˆå§‹åŒ–collectorï¼‰
    exits = [{'id': f'exit_{i}', 'position': np.array([0, 0])} for i in range(8)]
    
    collector = DensityDataCollector(
        exits=exits,
        save_dir=data_dir,
    )
    collector.load_all_episodes()
    
    # æ„å»ºæ•°æ®é›†
    print(f"\næ„å»ºæ•°æ®é›†...")
    print(f"  - åºåˆ—é•¿åº¦: {seq_length}å¸§ ({seq_length * 0.1}ç§’)")
    print(f"  - é¢„æµ‹æ­¥é•¿: {pred_horizon}å¸§ ({pred_horizon * 0.1}ç§’)")
    
    train_dataset, val_dataset = collector.build_dataset(
        seq_length=seq_length,
        pred_horizon=pred_horizon,
        stride=5,
        train_ratio=0.8,
    )
    
    train_loader = create_dataloader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = create_dataloader(val_dataset, batch_size=batch_size, shuffle=False)
    
    # åˆ›å»ºæ¨¡å‹
    print(f"\nåˆ›å»ºæ¨¡å‹...")
    if use_lite_model:
        model = DensityPredictorLite(
            input_channels=4,
            hidden_channels=32,
            grid_size=GRID_SIZE,
        )
        print("  - æ¨¡å‹ç±»å‹: Lite")
    else:
        model = DensityPredictorNet(
            input_channels=4,
            hidden_channels=64,
            encoder_channels=32,
            num_lstm_layers=2,
            grid_size=GRID_SIZE,
        )
        print("  - æ¨¡å‹ç±»å‹: Full")
    
    model = model.to(device)
    
    # è®¡ç®—å‚æ•°é‡
    n_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"  - å¯è®­ç»ƒå‚æ•°: {n_params:,}")
    
    # ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=5, verbose=True
    )
    criterion = nn.MSELoss()
    
    # è®­ç»ƒè®°å½•
    history = {
        'train_loss': [],
        'val_loss': [],
        'best_val_loss': float('inf'),
        'best_epoch': 0,
    }
    
    # è®­ç»ƒå¾ªç¯
    print(f"\nå¼€å§‹è®­ç»ƒ ({epochs} epochs)...")
    
    for epoch in range(epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_losses = []
        
        for batch_x, batch_y in tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs} [Train]"):
            batch_x = batch_x.to(device)
            batch_y = batch_y.to(device)
            
            optimizer.zero_grad()
            
            pred, _ = model(batch_x)
            loss = criterion(pred, batch_y)
            
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            
            train_losses.append(loss.item())
        
        avg_train_loss = np.mean(train_losses)
        history['train_loss'].append(avg_train_loss)
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_losses = []
        
        with torch.no_grad():
            for batch_x, batch_y in tqdm(val_loader, desc=f"Epoch {epoch+1}/{epochs} [Val]"):
                batch_x = batch_x.to(device)
                batch_y = batch_y.to(device)
                
                pred, _ = model(batch_x)
                loss = criterion(pred, batch_y)
                val_losses.append(loss.item())
        
        avg_val_loss = np.mean(val_losses)
        history['val_loss'].append(avg_val_loss)
        
        # å­¦ä¹ ç‡è°ƒæ•´
        scheduler.step(avg_val_loss)
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if avg_val_loss < history['best_val_loss']:
            history['best_val_loss'] = avg_val_loss
            history['best_epoch'] = epoch + 1
            
            # ä¿å­˜æ¨¡å‹
            Path(model_save_path).parent.mkdir(parents=True, exist_ok=True)
            torch.save(model.state_dict(), model_save_path)
            print(f"  [*] ä¿å­˜æœ€ä½³æ¨¡å‹ (val_loss: {avg_val_loss:.6f})")
        
        print(f"  Epoch {epoch+1}: train_loss={avg_train_loss:.6f}, val_loss={avg_val_loss:.6f}")
    
    # ä¿å­˜è®­ç»ƒå†å²
    history_path = Path(model_save_path).with_suffix('.json')
    with open(history_path, 'w') as f:
        json.dump({
            'train_loss': history['train_loss'],
            'val_loss': history['val_loss'],
            'best_val_loss': history['best_val_loss'],
            'best_epoch': history['best_epoch'],
            'config': {
                'epochs': epochs,
                'batch_size': batch_size,
                'learning_rate': learning_rate,
                'seq_length': seq_length,
                'pred_horizon': pred_horizon,
                'use_lite_model': use_lite_model,
            },
        }, f, indent=2)
    
    print("\n" + "=" * 60)
    print("è®­ç»ƒå®Œæˆ")
    print(f"  - æœ€ä½³éªŒè¯æŸå¤±: {history['best_val_loss']:.6f} (Epoch {history['best_epoch']})")
    print(f"  - æ¨¡å‹ä¿å­˜: {model_save_path}")
    print(f"  - å†å²ä¿å­˜: {history_path}")
    print("=" * 60)
    
    # åŠ è½½æœ€ä½³æ¨¡å‹
    model.load_state_dict(torch.load(model_save_path, map_location=device))
    
    return model


def evaluate_model(
    model_path: str = "outputs/models/density_predictor.pt",
    data_dir: str = "outputs/training_data",
    device: str = "auto",
    use_lite_model: bool = False,
):
    """è¯„ä¼°æ¨¡å‹æ€§èƒ½
    
    Args:
        model_path: æ¨¡å‹è·¯å¾„
        data_dir: æµ‹è¯•æ•°æ®ç›®å½•
        device: è®¾å¤‡
        use_lite_model: æ˜¯å¦ä¸ºè½»é‡çº§æ¨¡å‹
    """
    print("=" * 60)
    print("è¯„ä¼°æ¨¡å‹")
    print("=" * 60)
    
    # è®¾å¤‡æ£€æµ‹å’Œæ‰“å°
    print("\nè®¾å¤‡ä¿¡æ¯:")
    print(f"  - PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"  - CUDAå¯ç”¨: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"  - CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print(f"  - GPUæ•°é‡: {torch.cuda.device_count()}")
        for i in range(torch.cuda.device_count()):
            print(f"  - GPU {i}: {torch.cuda.get_device_name(i)}")
    
    if device == "auto":
        device = "cuda" if torch.cuda.is_available() else "cpu"
    
    print(f"\n  - ä½¿ç”¨è®¾å¤‡: {device}")
    if device == "cuda":
        print(f"  - å½“å‰GPU: {torch.cuda.get_device_name(0)}")
    print()
    
    # åŠ è½½æ¨¡å‹
    if use_lite_model:
        model = DensityPredictorLite(input_channels=4, hidden_channels=32, grid_size=GRID_SIZE)
    else:
        model = DensityPredictorNet(input_channels=4, hidden_channels=64, grid_size=GRID_SIZE)
    
    model.load_state_dict(torch.load(model_path, map_location=device))
    model = model.to(device)
    model.eval()
    print(f"åŠ è½½æ¨¡å‹: {model_path}")
    
    # åŠ è½½æ•°æ®
    exits = [{'id': f'exit_{i}', 'position': np.array([0, 0])} for i in range(8)]
    collector = DensityDataCollector(exits=exits, save_dir=data_dir)
    collector.load_all_episodes()
    
    _, test_dataset = collector.build_dataset(
        seq_length=10,
        pred_horizon=50,
        train_ratio=0.0,  # å…¨éƒ¨ç”¨äºæµ‹è¯•
    )
    
    test_loader = create_dataloader(test_dataset, batch_size=32, shuffle=False)
    
    # è¯„ä¼°
    criterion = nn.MSELoss()
    losses = []
    mae_values = []
    
    with torch.no_grad():
        for batch_x, batch_y in tqdm(test_loader, desc="Evaluating"):
            batch_x = batch_x.to(device)
            batch_y = batch_y.to(device)
            
            pred, _ = model(batch_x)
            
            loss = criterion(pred, batch_y)
            mae = torch.abs(pred - batch_y).mean()
            
            losses.append(loss.item())
            mae_values.append(mae.item())
    
    avg_mse = np.mean(losses)
    avg_mae = np.mean(mae_values)
    avg_rmse = np.sqrt(avg_mse)
    
    # è½¬æ¢ä¸ºå®é™…å¯†åº¦å€¼
    rmse_density = avg_rmse * MAX_SAFE_DENSITY
    mae_density = avg_mae * MAX_SAFE_DENSITY
    
    print("\n" + "=" * 60)
    print("è¯„ä¼°ç»“æœ")
    print("=" * 60)
    print(f"  MSE (å½’ä¸€åŒ–): {avg_mse:.6f}")
    print(f"  RMSE (å½’ä¸€åŒ–): {avg_rmse:.6f}")
    print(f"  MAE (å½’ä¸€åŒ–): {avg_mae:.6f}")
    print(f"  RMSE (äºº/mÂ²): {rmse_density:.4f}")
    print(f"  MAE (äºº/mÂ²): {mae_density:.4f}")
    print("=" * 60)


def main():
    parser = argparse.ArgumentParser(description="å¯†åº¦åœºé¢„æµ‹æ¨¡å‹è®­ç»ƒ")
    
    # æ“ä½œé€‰æ‹©
    parser.add_argument("--collect-data", action="store_true", help="æ”¶é›†è®­ç»ƒæ•°æ®")
    parser.add_argument("--train", action="store_true", help="è®­ç»ƒæ¨¡å‹")
    parser.add_argument("--evaluate", action="store_true", help="è¯„ä¼°æ¨¡å‹")
    
    # æ•°æ®æ”¶é›†å‚æ•°
    parser.add_argument("--n-episodes", type=int, default=10, help="æ”¶é›†çš„episodeæ•°é‡")
    parser.add_argument("--flow-level", type=str, default="small", 
                        choices=["small", "medium", "large"], help="äººæµé‡ç­‰çº§")
    parser.add_argument("--max-steps", type=int, default=3000, help="æ¯ä¸ªepisodeæœ€å¤§æ­¥æ•°")
    parser.add_argument("--collect-interval", type=int, default=5, 
                        help="æ•°æ®æ”¶é›†é—´éš”ï¼ˆæ¯Næ­¥æ”¶é›†ä¸€æ¬¡ï¼Œé»˜è®¤5æ­¥=0.5ç§’ï¼‰")
    parser.add_argument("--use-gpu-sfm", action="store_true", default=True,
                        help="ä½¿ç”¨GPUåŠ é€ŸSFMï¼ˆå¤§å¹…æå‡é€Ÿåº¦ï¼Œé»˜è®¤å¯ç”¨ï¼‰")
    parser.add_argument("--no-gpu-sfm", dest="use_gpu_sfm", action="store_false",
                        help="ç¦ç”¨GPUåŠ é€ŸSFMï¼ˆä½¿ç”¨CPUï¼Œè¾ƒæ…¢ï¼‰")
    parser.add_argument("--no-resume", dest="resume", action="store_false", default=True,
                        help="ç¦ç”¨æ–­ç‚¹ç»­è®­ï¼ˆä»å¤´å¼€å§‹æ”¶é›†ï¼‰")
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument("--epochs", type=int, default=50, help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--batch-size", type=int, default=32, help="æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--lr", type=float, default=1e-3, help="å­¦ä¹ ç‡")
    parser.add_argument("--seq-length", type=int, default=10, help="è¾“å…¥åºåˆ—é•¿åº¦")
    parser.add_argument("--pred-horizon", type=int, default=50, help="é¢„æµ‹æ­¥é•¿")
    parser.add_argument("--lite", action="store_true", help="ä½¿ç”¨è½»é‡çº§æ¨¡å‹")
    
    # è·¯å¾„
    parser.add_argument("--data-dir", type=str, default="outputs/training_data", help="æ•°æ®ç›®å½•")
    parser.add_argument("--model-path", type=str, default="outputs/models/density_predictor.pt", 
                        help="æ¨¡å‹è·¯å¾„")
    
    # è®¾å¤‡
    parser.add_argument("--device", type=str, default="auto", help="è®¡ç®—è®¾å¤‡")
    
    args = parser.parse_args()
    
    # åœ¨å¼€å§‹æ—¶æ‰“å°è®¾å¤‡ä¿¡æ¯
    print("\n" + "=" * 60)
    print("ç³»ç»Ÿè®¾å¤‡ä¿¡æ¯")
    print("=" * 60)
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print(f"GPUæ•°é‡: {torch.cuda.device_count()}")
        for i in range(torch.cuda.device_count()):
            props = torch.cuda.get_device_properties(i)
            print(f"  GPU {i}: {props.name}")
            print(f"    æ˜¾å­˜: {props.total_memory / 1024**3:.2f} GB")
            print(f"    è®¡ç®—èƒ½åŠ›: {props.major}.{props.minor}")
    else:
        print("  (å°†ä½¿ç”¨CPUè¿›è¡Œè®­ç»ƒ)")
    print("=" * 60 + "\n")
    
    # é»˜è®¤è¡Œä¸ºï¼šåŒæ—¶æ”¶é›†æ•°æ®å’Œè®­ç»ƒ
    if not args.collect_data and not args.train and not args.evaluate:
        args.collect_data = True
        args.train = True
    
    # æ”¶é›†æ•°æ®
    if args.collect_data:
        collect_training_data(
            n_episodes=args.n_episodes,
            flow_level=args.flow_level,
            max_steps=args.max_steps,
            save_dir=args.data_dir,
            collect_interval=args.collect_interval,
            use_gpu_sfm=args.use_gpu_sfm,
            resume=args.resume,
        )
    
    # è®­ç»ƒ
    if args.train:
        train_model(
            data_dir=args.data_dir,
            model_save_path=args.model_path,
            epochs=args.epochs,
            batch_size=args.batch_size,
            learning_rate=args.lr,
            seq_length=args.seq_length,
            pred_horizon=args.pred_horizon,
            use_lite_model=args.lite,
            device=args.device,
        )
    
    # è¯„ä¼°
    if args.evaluate:
        evaluate_model(
            model_path=args.model_path,
            data_dir=args.data_dir,
            device=args.device,
            use_lite_model=args.lite,
        )


if __name__ == "__main__":
    main()
